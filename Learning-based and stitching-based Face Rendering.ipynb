{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf6c172",
   "metadata": {},
   "source": [
    "# Capturing neutral face video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abba623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import face_alignment\n",
    "import time\n",
    "import cv2\n",
    "import scipy.optimize as opt\n",
    "from calibration.undistort import undistort\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv2.circle(image, (mark[0], mark[1]), 2, color, -1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device='cuda')\n",
    "\n",
    "# this function compares the differences between two sets of points\n",
    "# each column is a point\n",
    "def err(p1, p2):\n",
    "    # hopefully this is actually correct\n",
    "    return sum(sum((p1 - p2)*(p1 - p2)))\n",
    "\n",
    "\n",
    "\n",
    "# x is an array of 9 elements\n",
    "def to_optimize(x, original_points, target_points):\n",
    "    # reshape it to be a 3x3 transformation matrix\n",
    "    transform = x.reshape((3,3))\n",
    "    temp = transform@original_points\n",
    "    result = np.zeros((2,original_points.shape[1]))\n",
    "    for i in range(original_points.shape[1]):\n",
    "        result[0,i] = temp[0,i]/temp[2,i]\n",
    "        result[1,i] = temp[1,i]/temp[2,i]\n",
    "    return err(result, target_points[:2,:])\n",
    "\n",
    "\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "def capture_initial_video():\n",
    "    original_points = 0\n",
    "    target_points = 0\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    ## capture jawline with headset on and save the feature pts for alignment \n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imshow(winname=\"RAW FRAME\", mat=frame)\n",
    "\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv2.resize(frame,(256,256))\n",
    "        \n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            headset_features = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            target_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            target_points = np.append(target_points, np.ones((17,1)), axis=1)\n",
    "            target_points = target_points.T\n",
    "            print(\"got target points with shape\", target_points.shape)\n",
    "            break   \n",
    "        \n",
    "    video_out = cv2.VideoWriter('./init_video.mp4',cv2.VideoWriter_fourcc(*'XVID'),25,(256,256))\n",
    "    print('now trying to find a good no-headset match')\n",
    "    record = False\n",
    "    \n",
    "    ## align \n",
    "    result_marks = 0\n",
    "    generated_result = False\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv2.resize(frame,(256,256))\n",
    "        # show the image\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            new_face = preds[0].astype(int)\n",
    "            clean_frame = frame.copy()\n",
    "\n",
    "            draw_marks(frame,new_face,color = (0,0,255))\n",
    "        if generated_result:\n",
    "            print(\"generating result\")\n",
    "            draw_marks(frame,result_marks, color = (0,255,0))\n",
    "        draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "\n",
    "        pressed_key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        if(pressed_key == ord('q')):\n",
    "            print(\"stop recording\")\n",
    "            break\n",
    "        elif(pressed_key == ord('r')):\n",
    "            print('started recording')\n",
    "            record = True\n",
    "            original_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            original_points = np.append(original_points, np.ones((original_points.shape[0],1)), axis=1)\n",
    "            original_points = original_points.T\n",
    "            \n",
    "            result_marks = original_points\n",
    "            \n",
    "            generated_result = True\n",
    "\n",
    "        if(record):\n",
    "            video_out.write(clean_frame)\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    video_out.release()\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "    pickle.dump(clean_frame,open('./starting_picture.p','wb'))\n",
    "    return clean_frame\n",
    "\n",
    "print(\"run\")\n",
    "\n",
    "capture_initial_video()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "## Take the first frame of the video w/ headset on \n",
    "cap1 = cv2.VideoCapture('./init_video.mp4')\n",
    "for i in range(30):\n",
    "    res, frame = cap1.read()\n",
    "    cv2.imshow('aaaa',frame)\n",
    "    cv2.waitKey(100)\n",
    "cv2.imwrite('./init_img2.png',frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266747eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "import pdb\n",
    "#import pyvirtualcam\n",
    "import time\n",
    "from calibration.undistort import undistort\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "USE_RECORDED_VIDEO = True\n",
    "MERGE_HEIGHT = 128\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv2.circle(image, (mark[0], mark[1]), 2, color, -1, cv2.LINE_AA)    \n",
    "    \n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        kp_detector.cuda()\n",
    "    \n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    " \n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "    \n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector\n",
    "\n",
    "## make animation based on a video (sequence of frames), essentially the same as create_frame but for videos\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            \n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_frame(driving_video, generator, kp_detector,kp_source,kp_driving_initial,\\\n",
    "                 source, relative=True, adapt_movement_scale=True, cpu=False,):\n",
    "    with torch.no_grad():\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving_frame = torch.tensor(driving_video[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            driving_frame = driving_frame.cuda()\n",
    "        ## detect driving frame kp\n",
    "        kp_driving = kp_detector(driving_frame)\n",
    "        kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                               kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                               use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "        out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "        return np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "\n",
    "\n",
    "\n",
    "source_image = (imageio.imread('./init_img2.png')).astype(np.uint8)\n",
    "reader2 = cv2.VideoCapture('./init_video.mp4')\n",
    "driving_video = []\n",
    "regular_video = []\n",
    "\n",
    "def pre_process_frame(frame, crop = True, cvt_color = True):\n",
    "    #frame = frame[:,80:-80,:]\n",
    "    if crop:\n",
    "        frame = crop_image(frame)\n",
    "    frame = cv2.resize(frame,(256,256))\n",
    "    if cvt_color:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    frame = [resize(img, (256, 256))[..., :3] for img in [frame]]\n",
    "    return frame\n",
    "        \n",
    "while(1):\n",
    "    ret,frame = reader2.read()\n",
    "    if(ret):\n",
    "        regular_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "## first image of the natural blinking video\n",
    "source_image = resize(source_image, (256, 256))[..., :3]\n",
    "## resize the natural blinking video \n",
    "regular_video = [resize(frame, (256, 256))[..., :3] for frame in regular_video]\n",
    "\n",
    "if not USE_RECORDED_VIDEO:\n",
    "    ## make sure camera is working \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    res, frame = cap.read()\n",
    "    while(res is not True):\n",
    "        res, frame = cap.read()\n",
    "        print('retry')\n",
    "        time.sleep(0.3)\n",
    "    driving_video = pre_process_frame(frame)\n",
    "else:\n",
    "    saved_video_reader = cv2.VideoCapture('./video_headset_on.mp4')\n",
    "    saved_video = []\n",
    "    while(1):\n",
    "        ret,frame = saved_video_reader.read()\n",
    "        if(ret):\n",
    "            saved_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            break\n",
    "    driving_video = pre_process_frame(saved_video[0], crop =False, cvt_color = False)\n",
    "\n",
    "## Add the neutral blinking video to the top part \n",
    "driving_video[0][:MERGE_HEIGHT,:] = regular_video[0][:MERGE_HEIGHT,:]\n",
    "# cv2.imshow(winname=\"Face\", mat=cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "## Load the pre-trained first-order model\n",
    "generator, kp_detector = load_checkpoints(config_path='./config/vox-adv-256.yaml', \\\n",
    "                                          checkpoint_path='./vox-adv-cpk.pth.tar', cpu=False)\n",
    "\n",
    "## torchlize first frame of neutral face\n",
    "source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "source = source.cuda()\n",
    "## the kp of this frame, which remains to be same throughout online streaming \n",
    "kp_source = kp_detector(source)\n",
    "\n",
    "## Torchlize first frame of driving/online video \n",
    "driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "relative = True\n",
    "adapt_movement_scale = True\n",
    "\n",
    "predictions = []\n",
    "kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "\n",
    "## Stitching-based \n",
    "height_black_band = [105, 120]\n",
    "lateral_shift = 2\n",
    "left_right_black_size = 90\n",
    "bottom_black_size = 70\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "#with pyvirtualcam.Camera(width=256, height=256, fps=30,device='/dev/video5') as cam:  \n",
    "#while(1):\n",
    "frame_to_save_indeces = [0,10,20,30,40,60,70,80,90, 25, 50, 75, 100, 125, 150, 175, 250]\n",
    "while counter < 251: #len(saved_video):\n",
    "    start = time.time()\n",
    "    if not USE_RECORDED_VIDEO:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imwrite(\"normalframe.jpg\", frame);\n",
    "        frame = pre_process_frame(frame)\n",
    "    else:\n",
    "        frame = saved_video[counter%len(saved_video)]\n",
    "        frame = pre_process_frame(frame, crop = False, cvt_color = False)\n",
    "    \n",
    "    ## stitching-based\n",
    "    stitch_frame = copy.copy(frame[0])\n",
    "    stitch_frame[:height_black_band[0],:] = regular_video[counter%len(regular_video)][:height_black_band[0],:] #256 x 256\n",
    "\n",
    "    stitch_frame[height_black_band[0]:height_black_band[1],:] = np.zeros((height_black_band[1] - height_black_band[0], 256, 3))\n",
    "\n",
    "    ## shift chin \n",
    "    stitch_frame[height_black_band[1]:,left_right_black_size + lateral_shift:256 - left_right_black_size + lateral_shift,:] = \\\n",
    "        stitch_frame[height_black_band[1]:,left_right_black_size :256 - left_right_black_size,:]\n",
    "\n",
    "    stitch_frame[height_black_band[1]:256,0:left_right_black_size] = \\\n",
    "        np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "    stitch_frame[height_black_band[1]:256,-left_right_black_size:] = \\\n",
    "        np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "    stitch_frame[-bottom_black_size:,:] = np.zeros((bottom_black_size, 256,3))\n",
    "    if counter in frame_to_save_indeces:\n",
    "        tmp = cv2.resize(stitch_frame,(720,720))\n",
    "        tmp[:,:,[0,2]] = tmp[:,:,[2,0]]\n",
    "        cv2.imwrite(f'black_frame_{counter}.jpg', (tmp*255).astype(np.uint8))\n",
    "    \n",
    "    \n",
    "    \n",
    "    raw_frame = copy.copy(frame[0])\n",
    "    tmp = cv2.resize(raw_frame,(720,720))\n",
    "    tmp[:,:,[0,2]] = tmp[:,:,[2,0]]\n",
    "    if counter in frame_to_save_indeces:\n",
    "        cv2.imwrite(f'raw_frame_{counter}.jpg', (tmp*255).astype(np.uint8))\n",
    "        \n",
    "    ## Add the neutral blinking video to the top part \n",
    "    frame[0][:MERGE_HEIGHT,:] = regular_video[counter%len(regular_video)][:MERGE_HEIGHT,:]   \n",
    "    \n",
    "    output = create_frame(frame[0],generator,kp_detector,kp_source,kp_driving_initial,source,relative=True, adapt_movement_scale=True, cpu=False)\n",
    "    output255 = (output*255).astype(np.uint8)\n",
    "    \n",
    "    output255 = cv2.resize(output255, (720,720))\n",
    "    output255[:,:,[0,2]] = output255[:,:,[2,0]]\n",
    "    if counter in frame_to_save_indeces:\n",
    "        cv2.imwrite(f'learning_output_{counter}.jpg', output255)\n",
    "    #cv2.imshow('aaaaa',output255)\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "    ## debugging visualization\n",
    "    single_frame = frame[0]\n",
    "    #draw_marks(single_frame, kp_driving[0].astype(int))\n",
    "    tmp = cv2.resize(single_frame,(720,720))\n",
    "    tmp[:,:,[0,2]] = tmp[:,:,[2,0]] \n",
    "    #cv2.imshow(\"split frame\",tmp)\n",
    "\n",
    "    if counter in frame_to_save_indeces:\n",
    "        cv2.imwrite(f'stitched_frame_{counter}.jpg', (tmp*255).astype(np.uint8))\n",
    "        \n",
    "        \n",
    "    counter = (counter+1)%len(regular_video)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c69772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
