{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf6c172",
   "metadata": {},
   "source": [
    "# Capturing neutral face video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763db95",
   "metadata": {},
   "source": [
    "# Rendering face\n",
    "## Remember to only run this once the headset is on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abba623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import face_alignment\n",
    "import time\n",
    "import cv2\n",
    "import scipy.optimize as opt\n",
    "from calibration.undistort import undistort\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv.circle(image, (mark[0], mark[1]), 2, color, -1, cv.LINE_AA)\n",
    "        \n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device='cuda')\n",
    "\n",
    "# this function compares the differences between two sets of points\n",
    "# each column is a point\n",
    "def err(p1, p2):\n",
    "    # hopefully this is actually correct\n",
    "    return sum(sum((p1 - p2)*(p1 - p2)))\n",
    "\n",
    "\n",
    "\n",
    "# x is an array of 9 elements\n",
    "def to_optimize(x, original_points, target_points):\n",
    "    # reshape it to be a 3x3 transformation matrix\n",
    "    transform = x.reshape((3,3))\n",
    "    temp = transform@original_points\n",
    "    result = np.zeros((2,original_points.shape[1]))\n",
    "    for i in range(original_points.shape[1]):\n",
    "        result[0,i] = temp[0,i]/temp[2,i]\n",
    "        result[1,i] = temp[1,i]/temp[2,i]\n",
    "    return err(result, target_points[:2,:])\n",
    "\n",
    "\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "def capture_initial_video():\n",
    "    original_points = 0\n",
    "    target_points = 0\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    ## capture jawline with headset on and save the feature pts\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imshow(winname=\"RAW FRAME\", mat=frame)\n",
    "\n",
    "#         frame = undistort(frame)\n",
    "#         frame = frame[:,80:-80,:]\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv.resize(frame,(256,256))\n",
    "        \n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            headset_features = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            draw_marks(frame,headset_features,color = (255,0,0))\n",
    "#             print(\"feature dims\", headset_features.shape)\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            target_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            target_points = np.append(target_points, np.ones((target_points.shape[0],1)), axis=1)\n",
    "            target_points = target_points.T\n",
    "            print(\"got target points with shape\", target_points.shape)\n",
    "            break   \n",
    "        \n",
    "    video_out = cv2.VideoWriter('./init_video.mp4',cv2.VideoWriter_fourcc(*'XVID'),25,(256,256))\n",
    "    print('now trying to find a good no-headset match')\n",
    "    record = False\n",
    "    \n",
    "    ## align \n",
    "    result_marks = 0\n",
    "    generated_result = False\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        \n",
    "        frame = crop_image(frame)\n",
    "        frame = cv.resize(frame,(256,256))\n",
    "        # show the image\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            new_face = preds[0].astype(int)\n",
    "            clean_frame = frame.copy()\n",
    "\n",
    "            draw_marks(frame,new_face,color = (0,0,255))\n",
    "        if generated_result:\n",
    "            print(\"generating result\")\n",
    "            draw_marks(frame,result_marks, color = (0,255,0))\n",
    "        draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "\n",
    "        pressed_key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        if(pressed_key == ord('q')):\n",
    "            print(\"stop recording\")\n",
    "            break\n",
    "        elif(pressed_key == ord('r')):\n",
    "            print('started recording')\n",
    "            record = True\n",
    "            original_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            original_points = np.append(original_points, np.ones((original_points.shape[0],1)), axis=1)\n",
    "            original_points = original_points.T\n",
    "            print(original_points.shape)\n",
    "            res = opt.minimize(to_optimize, np.array([1,0,0,0,1,0,0,0,1]), args=(original_points, target_points))\n",
    "            transform = res.x.reshape((3,3))\n",
    "            temp = transform@original_points\n",
    "            result = np.zeros((2,original_points.shape[1]))\n",
    "            for i in range(original_points.shape[1]):\n",
    "                result[0,i] = temp[0,i]/temp[2,i]\n",
    "                result[1,i] = temp[1,i]/temp[2,i]\n",
    "            result_marks = result.T.astype(int)\n",
    "            generated_result = True\n",
    "            print(result_marks)\n",
    "        if(record):\n",
    "            video_out.write(clean_frame)\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    video_out.release()\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "    pickle.dump(clean_frame,open('./starting_picture.p','wb'))\n",
    "    return clean_frame\n",
    "\n",
    "print(\"run\")\n",
    "\n",
    "capture_initial_video()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424b884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c444756",
   "metadata": {},
   "source": [
    "# Attention: \n",
    "## Run this before the next cell:\n",
    "### https://stackoverflow.com/questions/70775129/runtimeerror-v4l2loopback-backend-stdexception-when-using-pyvirtualcam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f939dc3",
   "metadata": {},
   "source": [
    "!sudo modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=4 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266747eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no frame\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno frame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     75\u001b[0m frame[\u001b[38;5;241m0\u001b[39m][:height_black_band[\u001b[38;5;241m0\u001b[39m],:] \u001b[38;5;241m=\u001b[39m regular_video[counter\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(regular_video)][:height_black_band[\u001b[38;5;241m0\u001b[39m],:] \u001b[38;5;66;03m#256 x 256\u001b[39;00m\n\u001b[1;32m     77\u001b[0m frame[\u001b[38;5;241m0\u001b[39m][height_black_band[\u001b[38;5;241m0\u001b[39m]:height_black_band[\u001b[38;5;241m1\u001b[39m],:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height_black_band[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m height_black_band[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mpre_process_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_process_frame\u001b[39m(frame):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#frame = frame[:,80:-80,:]\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame,(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     41\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)    \n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mcrop_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop_image\u001b[39m(image):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#### import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "import pdb\n",
    "#import pyvirtualcam\n",
    "import time\n",
    "from calibration.undistort import undistort\n",
    "import cv2\n",
    "\n",
    "\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "\n",
    "\n",
    "reader2 = cv2.VideoCapture('./init_video.mp4')\n",
    "regular_video = []\n",
    "\n",
    "def pre_process_frame(frame):\n",
    "    #frame = frame[:,80:-80,:]\n",
    "    frame = crop_image(frame)\n",
    "    frame = cv2.resize(frame,(256,256))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    frame = [resize(img, (256, 256))[..., :3] for img in [frame]]\n",
    "    return frame\n",
    "\n",
    "        \n",
    "while(1):\n",
    "    ret,frame = reader2.read()\n",
    "    if(ret):\n",
    "        regular_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "regular_video = [resize(frame, (256, 256))[..., :3] for frame in regular_video]\n",
    "cap = cv2.VideoCapture(0)\n",
    "res, frame = cap.read()\n",
    "while(res is not True):\n",
    "    res, frame = cap.read()\n",
    "    print('retry')\n",
    "    time.sleep(0.3)\n",
    "\n",
    "height_black_band = [105, 120]\n",
    "lateral_shift = 2\n",
    "left_right_black_size = 90\n",
    "bottom_black_size = 70\n",
    "counter = 1\n",
    "\n",
    "\n",
    "while(counter < 30000):\n",
    "        start = time.time()\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "\n",
    "        frame = pre_process_frame(frame) \n",
    "        frame[0][:height_black_band[0],:] = regular_video[counter%len(regular_video)][:height_black_band[0],:] #256 x 256\n",
    "        \n",
    "        frame[0][height_black_band[0]:height_black_band[1],:] = np.zeros((height_black_band[1] - height_black_band[0], 256, 3))\n",
    "\n",
    "        ## shift chin \n",
    "        frame[0][height_black_band[1]:,left_right_black_size + lateral_shift:256 - left_right_black_size + lateral_shift,:] = \\\n",
    "            frame[0][height_black_band[1]:,left_right_black_size :256 - left_right_black_size,:]\n",
    "        \n",
    "        frame[0][height_black_band[1]:256,0:left_right_black_size] = \\\n",
    "            np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "        frame[0][height_black_band[1]:256,-left_right_black_size:] = \\\n",
    "            np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "        frame[0][-bottom_black_size:,:] = np.zeros((bottom_black_size, 256,3))\n",
    "\n",
    "#         frame[0][height_black_band[0]:height_black_band[1],:] = np.ones((height_black_band[1] - height_black_band[0], 256, 3))*0.5\n",
    "\n",
    "#         ## shift chin \n",
    "#         frame[0][height_black_band[1]:,left_right_black_size + lateral_shift:256 - left_right_black_size + lateral_shift,:] = \\\n",
    "#             frame[0][height_black_band[1]:,left_right_black_size :256 - left_right_black_size,:]\n",
    "        \n",
    "#         frame[0][height_black_band[1]:256,0:left_right_black_size] = \\\n",
    "#             np.ones((256 - height_black_band[1], left_right_black_size, 3))*0.5\n",
    "#         frame[0][height_black_band[1]:256,-left_right_black_size:] = \\\n",
    "#             np.ones((256 - height_black_band[1], left_right_black_size, 3))*0.5\n",
    "#         frame[0][-bottom_black_size:,:] = np.ones((bottom_black_size, 256,3))*0.5\n",
    "\n",
    "        tmp = cv2.resize(frame[0], (720,720))\n",
    "        tmp[:,:,[0,2]] = tmp[:,:,[2,0]]\n",
    "        cv2.imshow('Stitched',tmp)\n",
    "        cv2.waitKey(1)\n",
    "        counter += 1\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11fb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for joao: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = 'sudo -S modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=5 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2'\n",
    "\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c68f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no frame\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno frame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m frame[\u001b[38;5;241m0\u001b[39m][:height_black_band[\u001b[38;5;241m0\u001b[39m],:] \u001b[38;5;241m=\u001b[39m regular_video[counter\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(regular_video)][:height_black_band[\u001b[38;5;241m0\u001b[39m],:] \u001b[38;5;66;03m#256 x 256\u001b[39;00m\n\u001b[1;32m     11\u001b[0m frame[\u001b[38;5;241m0\u001b[39m][height_black_band[\u001b[38;5;241m0\u001b[39m]:height_black_band[\u001b[38;5;241m1\u001b[39m],:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height_black_band[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m height_black_band[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn [2], line 39\u001b[0m, in \u001b[0;36mpre_process_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_process_frame\u001b[39m(frame):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#frame = frame[:,80:-80,:]\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame,(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     41\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)    \n",
      "Cell \u001b[0;32mIn [2], line 27\u001b[0m, in \u001b[0;36mcrop_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop_image\u001b[39m(image):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "       \n",
    "with pyvirtualcam.Camera(width=256, height=256, fps=30,device='/dev/video5') as cam:\n",
    "    while(1):\n",
    "        start = time.time()\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "\n",
    "        frame = pre_process_frame(frame) \n",
    "        frame[0][:height_black_band[0],:] = regular_video[counter%len(regular_video)][:height_black_band[0],:] #256 x 256\n",
    "        \n",
    "        frame[0][height_black_band[0]:height_black_band[1],:] = np.zeros((height_black_band[1] - height_black_band[0], 256, 3))\n",
    "        ## shift chin \n",
    "        frame[0][height_black_band[1]:,left_right_black_size + lateral_shift:256 - left_right_black_size + lateral_shift,:] = \\\n",
    "            frame[0][height_black_band[1]:,left_right_black_size :256 - left_right_black_size,:]\n",
    "        frame[0][height_black_band[1]:256,0:left_right_black_size] = np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "        frame[0][height_black_band[1]:256,-left_right_black_size:] = np.zeros((256 - height_black_band[1], left_right_black_size, 3))\n",
    "        frame[0][-bottom_black_size:,:] = np.zeros((bottom_black_size, 256,3))\n",
    "  \n",
    "    \n",
    "    \n",
    "#         frame[0][height_black_band[0]:height_black_band[1],:] = np.ones((height_black_band[1] - height_black_band[0], 256, 3))\n",
    "        \n",
    "#         ## shift chin \n",
    "#         frame[0][height_black_band[1]:,left_right_black_size + lateral_shift:256 - left_right_black_size + lateral_shift,:] = \\\n",
    "#             frame[0][height_black_band[1]:,left_right_black_size :256 - left_right_black_size,:]\n",
    "        \n",
    "#         frame[0][height_black_band[1]:256,0:left_right_black_size] = np.ones((256 - height_black_band[1], left_right_black_size, 3))\n",
    "#         frame[0][height_black_band[1]:256,-left_right_black_size:] = np.ones((256 - height_black_band[1], left_right_black_size, 3))\n",
    "#         frame[0][-bottom_black_size:,:] = np.ones((bottom_black_size, 256,3))\n",
    "        \n",
    "\n",
    "        cam.send((frame[0]*255).astype(np.uint8))\n",
    "        cam.sleep_until_next_frame()\n",
    "        counter = (counter+1)%len(regular_video)\n",
    "\n",
    "\n",
    "# # predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False)\n",
    "# imageio.mimsave('res.mp4', [img_as_ubyte(frame) for frame in predictions], fps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0ba28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
