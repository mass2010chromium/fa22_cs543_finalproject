{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf6c172",
   "metadata": {},
   "source": [
    "# Capturing neutral face video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763db95",
   "metadata": {},
   "source": [
    "# Rendering face\n",
    "## Remember to only run this once the headset is on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abba623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import face_alignment\n",
    "import time\n",
    "import cv2\n",
    "import scipy.optimize as opt\n",
    "from calibration.undistort import undistort\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv.circle(image, (mark[0], mark[1]), 2, color, -1, cv.LINE_AA)\n",
    "        \n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device='cuda')\n",
    "\n",
    "# this function compares the differences between two sets of points\n",
    "# each column is a point\n",
    "def err(p1, p2):\n",
    "    # hopefully this is actually correct\n",
    "    return sum(sum((p1 - p2)*(p1 - p2)))\n",
    "\n",
    "\n",
    "\n",
    "# x is an array of 9 elements\n",
    "def to_optimize(x, original_points, target_points):\n",
    "    # reshape it to be a 3x3 transformation matrix\n",
    "    transform = x.reshape((3,3))\n",
    "    temp = transform@original_points\n",
    "    result = np.zeros((2,original_points.shape[1]))\n",
    "    for i in range(original_points.shape[1]):\n",
    "        result[0,i] = temp[0,i]/temp[2,i]\n",
    "        result[1,i] = temp[1,i]/temp[2,i]\n",
    "    return err(result, target_points[:2,:])\n",
    "\n",
    "\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "def capture_initial_video():\n",
    "    original_points = 0\n",
    "    target_points = 0\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    ## capture jawline with headset on and save the feature pts\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imshow(winname=\"RAW FRAME\", mat=frame)\n",
    "\n",
    "#         frame = undistort(frame)\n",
    "#         frame = frame[:,80:-80,:]\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv.resize(frame,(256,256))\n",
    "        \n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            headset_features = preds[0].astype(int)[:17]\n",
    "            draw_marks(frame,headset_features,color = (255,0,0))\n",
    "#             print(\"feature dims\", headset_features.shape)\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            target_points = preds[0].astype(int)[:17]\n",
    "            target_points = np.append(target_points, np.ones((17,1)), axis=1)\n",
    "            target_points = target_points.T\n",
    "            print(\"got target points with shape\", target_points.shape)\n",
    "            break   \n",
    "        \n",
    "    video_out = cv2.VideoWriter('./init_video.mp4',cv2.VideoWriter_fourcc(*'XVID'),25,(256,256))\n",
    "    print('now trying to find a good no-headset match')\n",
    "    record = False\n",
    "    \n",
    "    ## align \n",
    "    result_marks = 0\n",
    "    generated_result = False\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "            \n",
    "#         print(frame.shape)\n",
    "#         cv2.imshow(winname = \"original Frame\",mat = cv2.resize(frame,(256,256)))\n",
    "\n",
    "        # Convert image into grayscale\n",
    "    #     frame = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2RGB)\n",
    "#         frame = undistort(frame)\n",
    "#         frame = frame[:,80:-80,:]\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv.resize(frame,(256,256))\n",
    "        # show the image\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            new_face = preds[0].astype(int)\n",
    "            clean_frame = frame.copy()\n",
    "\n",
    "            draw_marks(frame,new_face,color = (0,0,255))\n",
    "        if generated_result:\n",
    "            print(\"generating result\")\n",
    "            draw_marks(frame,result_marks, color = (0,255,0))\n",
    "        draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "\n",
    "        pressed_key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        if(pressed_key == ord('q')):\n",
    "            print(\"stop recording\")\n",
    "            break\n",
    "        elif(pressed_key == ord('r')):\n",
    "            print('started recording')\n",
    "            record = True\n",
    "            original_points = preds[0].astype(int)[:17]\n",
    "            original_points = np.append(original_points, np.ones((17,1)), axis=1)\n",
    "            original_points = original_points.T\n",
    "            print(original_points.shape)\n",
    "            res = opt.minimize(to_optimize, np.array([1,0,0,0,1,0,0,0,1]), args=(original_points, target_points))\n",
    "            transform = res.x.reshape((3,3))\n",
    "            temp = transform@original_points\n",
    "            result = np.zeros((2,original_points.shape[1]))\n",
    "            for i in range(original_points.shape[1]):\n",
    "                result[0,i] = temp[0,i]/temp[2,i]\n",
    "                result[1,i] = temp[1,i]/temp[2,i]\n",
    "            result_marks = result.T.astype(int)\n",
    "            generated_result = True\n",
    "            print(result_marks)\n",
    "        if(record):\n",
    "            video_out.write(clean_frame)\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    video_out.release()\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "    pickle.dump(clean_frame,open('./starting_picture.p','wb'))\n",
    "    return clean_frame\n",
    "\n",
    "print(\"run\")\n",
    "\n",
    "capture_initial_video()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "## Take the first frame of the video w/ headset on \n",
    "cap1 = cv2.VideoCapture('./init_video.mp4')\n",
    "for i in range(30):\n",
    "    res, frame = cap1.read()\n",
    "    cv2.imshow('aaaa',frame)\n",
    "    cv2.waitKey(100)\n",
    "cv2.imwrite('./init_img2.png',frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c444756",
   "metadata": {},
   "source": [
    "# Attention: \n",
    "## Run this before the next cell:\n",
    "### https://stackoverflow.com/questions/70775129/runtimeerror-v4l2loopback-backend-stdexception-when-using-pyvirtualcam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f939dc3",
   "metadata": {},
   "source": [
    "!sudo modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=4 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = 'sudo -S modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=5 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2'\n",
    "\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266747eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "import pdb\n",
    "import pyvirtualcam\n",
    "import time\n",
    "from calibration.undistort import undistort\n",
    "import cv2\n",
    "\n",
    "\n",
    "MERGE_HEIGHT = 150\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        kp_detector.cuda()\n",
    "    \n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    " \n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "    \n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector\n",
    "\n",
    "\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "#             pdb.set_trace()\n",
    "#             print(kp_driving.shape)\n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "def find_best_frame(source, driving, cpu=False):\n",
    "    import face_alignment\n",
    "\n",
    "    def normalize_kp(kp):\n",
    "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
    "        area = ConvexHull(kp[:, :2]).volume\n",
    "        area = np.sqrt(area)\n",
    "        kp[:, :2] = kp[:, :2] / area\n",
    "        return kp\n",
    "\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n",
    "                                      device='cpu' if cpu else 'cuda')\n",
    "    kp_source = fa.get_landmarks(255 * source)[0]\n",
    "    kp_source = normalize_kp(kp_source)\n",
    "    norm  = float('inf')\n",
    "    frame_num = 0\n",
    "    for i, image in tqdm(enumerate(driving)):\n",
    "        kp_driving = fa.get_landmarks(255 * image)[0]\n",
    "        kp_driving = normalize_kp(kp_driving)\n",
    "        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n",
    "        if new_norm < norm:\n",
    "            norm = new_norm\n",
    "            frame_num = i\n",
    "    return frame_num\n",
    "\n",
    "def create_frame(source_image, driving_video, generator, kp_detector,kp_source,kp_driving_initial,source, relative=True, adapt_movement_scale=True, cpu=False,):\n",
    "    with torch.no_grad():\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving_frame = torch.tensor(driving_video[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            driving_frame = driving_frame.cuda()\n",
    "        kp_driving = kp_detector(driving_frame)\n",
    "        kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                               kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                               use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "        out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "        return np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "\n",
    "\n",
    "\n",
    "source_image = (imageio.imread('./init_img2.png')).astype(np.uint8)\n",
    "# reader = cv2.VideoCapture('./test_video.mp4')\n",
    "reader2 = cv2.VideoCapture('./init_video.mp4')\n",
    "# reader = imageio.get_reader('./test_video.mp4')\n",
    "# reader2 = imageio.get_reader('./init_video.mp4')\n",
    "# fps = reader.get_meta_data()['fps']\n",
    "driving_video = []\n",
    "regular_video = []\n",
    "\n",
    "def pre_process_frame(frame):\n",
    "    #frame = frame[:,80:-80,:]\n",
    "    frame = crop_image(frame)\n",
    "    frame = cv2.resize(frame,(256,256))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    frame = [resize(img, (256, 256))[..., :3] for img in [frame]]\n",
    "    return frame\n",
    "\n",
    "        \n",
    "while(1):\n",
    "    ret,frame = reader2.read()\n",
    "    if(ret):\n",
    "        regular_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "source_image = resize(source_image, (256, 256))[..., :3]\n",
    "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "regular_video = [resize(frame, (256, 256))[..., :3] for frame in regular_video]\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "res, frame = cap.read()\n",
    "\n",
    "while(res is not True):\n",
    "    res, frame = cap.read()\n",
    "    print('retry')\n",
    "    time.sleep(0.3)\n",
    "# frame = undistort(frame)\n",
    "driving_video = pre_process_frame(frame)\n",
    "\n",
    "driving_video[0][:MERGE_HEIGHT,:] = regular_video[0][:MERGE_HEIGHT,:]\n",
    "\n",
    "# cv2.imshow(winname=\"Face\", mat=cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "generator, kp_detector = load_checkpoints(config_path='./config/vox-adv-256.yaml', checkpoint_path='./vox-adv-cpk.pth.tar', cpu=False)\n",
    "\n",
    "source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "source = source.cuda()\n",
    "# driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "kp_source = kp_detector(source)\n",
    "\n",
    "driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "relative = True\n",
    "adapt_movement_scale = True\n",
    "\n",
    "predictions = []\n",
    "\n",
    "\n",
    "kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "counter = 0\n",
    "\n",
    "with pyvirtualcam.Camera(width=256, height=256, fps=30,device='/dev/video5') as cam:\n",
    "    while(1):\n",
    "        start = time.time()\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "#         frame = undistort(frame)\n",
    "        cv2.imwrite(\"normalframe.jpg\", frame);\n",
    "        frame = pre_process_frame(frame)\n",
    "        frame[0][:128,:] = regular_video[counter%len(regular_video)][:128,:]\n",
    "        cv2.imshow(\"split frame\",cv2.resize(frame[0],(720,720)))\n",
    "#         print(frame)\n",
    "#         cv2.imshow('stitched',mat = frame[0])\n",
    "#         plt.imshow(frame[0])\n",
    "#         plt.show()\n",
    "        output = create_frame(source_image,frame[0],generator,kp_detector,kp_source,kp_driving_initial,source,relative=True, adapt_movement_scale=True, cpu=False)\n",
    "        output255 = (output*255).astype(np.uint8)\n",
    "        cam.send(output255)\n",
    "#         cv2.imwrite(\"outputframe.jpg\", output255)\n",
    "#         cv2.imshow('aaaaa',output255)\n",
    "#         cv2.imshow('original',frame)\n",
    "#         cv2.waitKey(1)\n",
    "        cam.sleep_until_next_frame()\n",
    "        counter = (counter+1)%len(regular_video)\n",
    "#         cv2.waitKey(5)\n",
    "#     predictions.append(output)\n",
    "\n",
    "# predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False)\n",
    "imageio.mimsave('res.mp4', [img_as_ubyte(frame) for frame in predictions], fps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6,7,8,9])\n",
    "print(x.reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c69772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
