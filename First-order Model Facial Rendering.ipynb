{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf6c172",
   "metadata": {},
   "source": [
    "# Capturing neutral face video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763db95",
   "metadata": {},
   "source": [
    "# Rendering face\n",
    "## Remember to only run this once the headset is on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abba623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/venv/lib/python3.8/site-packages/face_alignment/api.py:146: UserWarning: No faces were detected.\n",
      "  warnings.warn(\"No faces were detected.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n",
      "??????\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_frame\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mcapture_initial_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mcapture_initial_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     draw_marks(frame,headset_features,color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     77\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(winname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace\u001b[39m\u001b[38;5;124m\"\u001b[39m, mat\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mresize(frame,(\u001b[38;5;241m720\u001b[39m,\u001b[38;5;241m720\u001b[39m)))\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     79\u001b[0m     target_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)[:\u001b[38;5;241m17\u001b[39m],preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)[\u001b[38;5;241m49\u001b[39m:\u001b[38;5;241m61\u001b[39m]))\n\u001b[1;32m     80\u001b[0m     target_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(target_points, np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import face_alignment\n",
    "import time\n",
    "import cv2\n",
    "import scipy.optimize as opt\n",
    "from calibration.undistort import undistort\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv2.circle(image, (mark[0], mark[1]), 2, color, -1, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device='cuda')\n",
    "\n",
    "# this function compares the differences between two sets of points\n",
    "# each column is a point\n",
    "def err(p1, p2):\n",
    "    # hopefully this is actually correct\n",
    "    return sum(sum((p1 - p2)*(p1 - p2)))\n",
    "\n",
    "\n",
    "\n",
    "# x is an array of 9 elements\n",
    "def to_optimize(x, original_points, target_points):\n",
    "    # reshape it to be a 3x3 transformation matrix\n",
    "    transform = x.reshape((3,3))\n",
    "    temp = transform@original_points\n",
    "    result = np.zeros((2,original_points.shape[1]))\n",
    "    for i in range(original_points.shape[1]):\n",
    "        result[0,i] = temp[0,i]/temp[2,i]\n",
    "        result[1,i] = temp[1,i]/temp[2,i]\n",
    "    return err(result, target_points[:2,:])\n",
    "\n",
    "\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "def capture_initial_video():\n",
    "    original_points = 0\n",
    "    target_points = 0\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    ## capture jawline with headset on and save the feature pts for alignment \n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imshow(winname=\"RAW FRAME\", mat=frame)\n",
    "\n",
    "#         frame = undistort(frame)\n",
    "#         frame = frame[:,80:-80,:]\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv2.resize(frame,(256,256))\n",
    "        \n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            headset_features = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            target_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            target_points = np.append(target_points, np.ones((17,1)), axis=1)\n",
    "            target_points = target_points.T\n",
    "            print(\"got target points with shape\", target_points.shape)\n",
    "            break   \n",
    "        \n",
    "    video_out = cv2.VideoWriter('./init_video.mp4',cv2.VideoWriter_fourcc(*'XVID'),25,(256,256))\n",
    "    print('now trying to find a good no-headset match')\n",
    "    record = False\n",
    "    \n",
    "    ## align \n",
    "    result_marks = 0\n",
    "    generated_result = False\n",
    "    while True:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "\n",
    "        frame = crop_image(frame)\n",
    "        frame = cv2.resize(frame,(256,256))\n",
    "        # show the image\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if(preds):\n",
    "            new_face = preds[0].astype(int)\n",
    "            clean_frame = frame.copy()\n",
    "\n",
    "            draw_marks(frame,new_face,color = (0,0,255))\n",
    "        if generated_result:\n",
    "            print(\"generating result\")\n",
    "            draw_marks(frame,result_marks, color = (0,255,0))\n",
    "        draw_marks(frame,headset_features,color = (255,0,0))\n",
    "        cv2.imshow(winname=\"Face\", mat=cv2.resize(frame,(720,720)))\n",
    "\n",
    "        pressed_key = cv2.waitKey(5) & 0xFF\n",
    "\n",
    "        if(pressed_key == ord('q')):\n",
    "            print(\"stop recording\")\n",
    "            break\n",
    "        elif(pressed_key == ord('r')):\n",
    "            print('started recording')\n",
    "            record = True\n",
    "            original_points = np.concatenate((preds[0].astype(int)[:17],preds[0].astype(int)[49:61]))\n",
    "            original_points = np.append(original_points, np.ones((original_points.shape[0],1)), axis=1)\n",
    "            original_points = original_points.T\n",
    "            \n",
    "            \n",
    "            ### This alignment procedure seems to be in the wrong place. \n",
    "            ## align the new \n",
    "#             res = opt.minimize(to_optimize, np.array([1,0,0,0,1,0,0,0,1]), args=(original_points, target_points))\n",
    "#             transform = res.x.reshape((3,3))\n",
    "#             temp = transform@original_points\n",
    "#             result = np.zeros((2,original_points.shape[1]))\n",
    "#             for i in range(original_points.shape[1]):\n",
    "#                 result[0,i] = temp[0,i]/temp[2,i]\n",
    "#                 result[1,i] = temp[1,i]/temp[2,i]\n",
    "#             result_marks = result.T.astype(int)\n",
    "            \n",
    "            result_marks = original_points\n",
    "            \n",
    "            generated_result = True\n",
    "\n",
    "        if(record):\n",
    "            video_out.write(clean_frame)\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cap.release()\n",
    "    video_out.release()\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "    pickle.dump(clean_frame,open('./starting_picture.p','wb'))\n",
    "    return clean_frame\n",
    "\n",
    "print(\"run\")\n",
    "\n",
    "capture_initial_video()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "## Take the first frame of the video w/ headset on \n",
    "cap1 = cv2.VideoCapture('./init_video.mp4')\n",
    "for i in range(30):\n",
    "    res, frame = cap1.read()\n",
    "    cv2.imshow('aaaa',frame)\n",
    "    cv2.waitKey(100)\n",
    "cv2.imwrite('./init_img2.png',frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c444756",
   "metadata": {},
   "source": [
    "# Attention: \n",
    "## Run this before the next cell:\n",
    "### https://stackoverflow.com/questions/70775129/runtimeerror-v4l2loopback-backend-stdexception-when-using-pyvirtualcam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f939dc3",
   "metadata": {},
   "source": [
    "!sudo modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=4 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = 'sudo -S modprobe -r v4l2loopback && sudo modprobe v4l2loopback devices=1 video_nr=5 card_label=\"Virtual\" exclusive_caps=1 max_buffers=2'\n",
    "\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266747eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/venv/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/yifan/venv/lib/python3.8/site-packages/torch/nn/functional.py:4193: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/yifan/venv/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m## Add the neutral blinking video to the top part \u001b[39;00m\n\u001b[1;32m    215\u001b[0m frame[\u001b[38;5;241m0\u001b[39m][:MERGE_HEIGHT,:] \u001b[38;5;241m=\u001b[39m regular_video[counter\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(regular_video)][:MERGE_HEIGHT,:]   \n\u001b[0;32m--> 217\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkp_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkp_driving_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrelative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapt_movement_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m output255 \u001b[38;5;241m=\u001b[39m (output\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    220\u001b[0m output255 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(output255, (\u001b[38;5;241m720\u001b[39m,\u001b[38;5;241m720\u001b[39m))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mcreate_frame\u001b[0;34m(driving_video, generator, kp_detector, kp_source, kp_driving_initial, source, relative, adapt_movement_scale, cpu)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m## detect driving frame kp\u001b[39;00m\n\u001b[1;32m    117\u001b[0m kp_driving \u001b[38;5;241m=\u001b[39m kp_detector(driving_frame)\n\u001b[0;32m--> 118\u001b[0m kp_norm \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_kp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkp_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_driving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkp_driving\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkp_driving_initial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkp_driving_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_relative_movement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                       \u001b[49m\u001b[43muse_relative_jacobian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapt_movement_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapt_movement_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m out \u001b[38;5;241m=\u001b[39m generator(source, kp_source\u001b[38;5;241m=\u001b[39mkp_source, kp_driving\u001b[38;5;241m=\u001b[39mkp_norm)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/fa22_cs543_finalproject/animate.py:19\u001b[0m, in \u001b[0;36mnormalize_kp\u001b[0;34m(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale, use_relative_movement, use_relative_jacobian)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_kp\u001b[39m(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m                  use_relative_movement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_relative_jacobian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapt_movement_scale:\n\u001b[0;32m---> 19\u001b[0m         source_area \u001b[38;5;241m=\u001b[39m ConvexHull(\u001b[43mkp_source\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mvolume\n\u001b[1;32m     20\u001b[0m         driving_area \u001b[38;5;241m=\u001b[39m ConvexHull(kp_driving_initial[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mvolume\n\u001b[1;32m     21\u001b[0m         adapt_movement_scale \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(source_area) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(driving_area)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "import pdb\n",
    "#import pyvirtualcam\n",
    "import time\n",
    "from calibration.undistort import undistort\n",
    "import cv2\n",
    "\n",
    "USE_RECORDED_VIDEO = True\n",
    "MERGE_HEIGHT = 128\n",
    "def crop_image(image):\n",
    "    return image[:,80:-80,:]\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "\n",
    "def draw_marks(image, marks, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw the facial landmarks on an image\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.uint8\n",
    "        Image on which landmarks are to be drawn.\n",
    "    marks : list or numpy array\n",
    "        Facial landmark points\n",
    "    color : tuple, optional\n",
    "        Color to which landmarks are to be drawn with. The default is (0, 255, 0).\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    for mark in marks:\n",
    "        cv2.circle(image, (mark[0], mark[1]), 2, color, -1, cv2.LINE_AA)    \n",
    "    \n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    if not cpu:\n",
    "        kp_detector.cuda()\n",
    "    \n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    " \n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "    \n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "    generator.eval()\n",
    "    kp_detector.eval()\n",
    "    \n",
    "    return generator, kp_detector\n",
    "\n",
    "## make animation based on a video (sequence of frames), essentially the same as create_frame but for videos\n",
    "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            \n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_frame(driving_video, generator, kp_detector,kp_source,kp_driving_initial,\\\n",
    "                 source, relative=True, adapt_movement_scale=True, cpu=False,):\n",
    "    with torch.no_grad():\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving_frame = torch.tensor(driving_video[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            driving_frame = driving_frame.cuda()\n",
    "        ## detect driving frame kp\n",
    "        kp_driving = kp_detector(driving_frame)\n",
    "        kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                               kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                               use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "        out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n",
    "        return np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "\n",
    "\n",
    "\n",
    "source_image = (imageio.imread('./init_img2.png')).astype(np.uint8)\n",
    "reader2 = cv2.VideoCapture('./init_video.mp4')\n",
    "driving_video = []\n",
    "regular_video = []\n",
    "\n",
    "def pre_process_frame(frame, crop = True, cvt_color = True):\n",
    "    #frame = frame[:,80:-80,:]\n",
    "    if crop:\n",
    "        frame = crop_image(frame)\n",
    "    frame = cv2.resize(frame,(256,256))\n",
    "    if cvt_color:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    frame = [resize(img, (256, 256))[..., :3] for img in [frame]]\n",
    "    return frame\n",
    "        \n",
    "while(1):\n",
    "    ret,frame = reader2.read()\n",
    "    if(ret):\n",
    "        regular_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "## first image of the natural blinking video\n",
    "source_image = resize(source_image, (256, 256))[..., :3]\n",
    "## resize the natural blinking video \n",
    "regular_video = [resize(frame, (256, 256))[..., :3] for frame in regular_video]\n",
    "\n",
    "if not USE_RECORDED_VIDEO:\n",
    "    ## make sure camera is working \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    res, frame = cap.read()\n",
    "    while(res is not True):\n",
    "        res, frame = cap.read()\n",
    "        print('retry')\n",
    "        time.sleep(0.3)\n",
    "    driving_video = pre_process_frame(frame)\n",
    "else:\n",
    "    saved_video_reader = cv2.VideoCapture('./video_headset_on.mp4')\n",
    "    saved_video = []\n",
    "    while(1):\n",
    "        ret,frame = saved_video_reader.read()\n",
    "        if(ret):\n",
    "            saved_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            break\n",
    "    driving_video = pre_process_frame(saved_video[0], crop =False, cvt_color = False)\n",
    "\n",
    "## Add the neutral blinking video to the top part \n",
    "driving_video[0][:MERGE_HEIGHT,:] = regular_video[0][:MERGE_HEIGHT,:]\n",
    "# cv2.imshow(winname=\"Face\", mat=cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "## Load the pre-trained first-order model\n",
    "generator, kp_detector = load_checkpoints(config_path='./config/vox-adv-256.yaml', \\\n",
    "                                          checkpoint_path='./vox-adv-cpk.pth.tar', cpu=False)\n",
    "\n",
    "## torchlize first frame of neutral face\n",
    "source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "source = source.cuda()\n",
    "## the kp of this frame, which remains to be same throughout online streaming \n",
    "kp_source = kp_detector(source)\n",
    "\n",
    "## Torchlize first frame of driving/online video \n",
    "driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "relative = True\n",
    "adapt_movement_scale = True\n",
    "\n",
    "predictions = []\n",
    "kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "#with pyvirtualcam.Camera(width=256, height=256, fps=30,device='/dev/video5') as cam:    \n",
    "while(1):\n",
    "    start = time.time()\n",
    "    if not USE_RECORDED_VIDEO:\n",
    "        res, frame = cap.read()\n",
    "        if not res:\n",
    "            print('no frame')\n",
    "            continue\n",
    "        cv2.imwrite(\"normalframe.jpg\", frame);\n",
    "        frame = pre_process_frame(frame)\n",
    "    else:\n",
    "        frame = saved_video[counter%len(saved_video)]\n",
    "        frame = pre_process_frame(frame, crop = False, cvt_color = False)\n",
    "        \n",
    "    ## Add the neutral blinking video to the top part \n",
    "    frame[0][:MERGE_HEIGHT,:] = regular_video[counter%len(regular_video)][:MERGE_HEIGHT,:]   \n",
    "    \n",
    "    output = create_frame(frame[0],generator,kp_detector,kp_source,kp_driving_initial,source,relative=True, adapt_movement_scale=True, cpu=False)\n",
    "    output255 = (output*255).astype(np.uint8)\n",
    "    \n",
    "    output255 = cv2.resize(output255, (720,720))\n",
    "    output255[:,:,[0,2]] = output255[:,:,[2,0]]\n",
    "    #cv2.imwrite(\"outputframe.jpg\", output255)\n",
    "    cv2.imshow('aaaaa',output255)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    ## debugging visualization\n",
    "    single_frame = frame[0]\n",
    "    #draw_marks(single_frame, kp_driving[0].astype(int))\n",
    "    tmp = cv2.resize(single_frame,(720,720))\n",
    "    tmp[:,:,[0,2]] = tmp[:,:,[2,0]] \n",
    "    cv2.imshow(\"split frame\",tmp)\n",
    "    \n",
    "    \n",
    "    counter = (counter+1)%len(regular_video)\n",
    "    \n",
    "    \n",
    "#         cv2.waitKey(5)\n",
    "#     predictions.append(output)\n",
    "\n",
    "# predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False)\n",
    "imageio.mimsave('res.mp4', [img_as_ubyte(frame) for frame in predictions], fps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "print(yaml.__file__)\n",
    "yaml.load('./config/vox-adv-256.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c69772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
